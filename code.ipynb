{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.util import save_json, now\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "IMDB=pd.read_csv(\"./database/IMDB Dataset.csv\")\n",
    "IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDB.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDB.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA 진행 결과 IMDB 데이터 셋의 열은 review와 sentiment로 구성,\n",
    "각 열에 해당되는 행의 개수는 총 50000개이며 고유값들은 각각 49582개, 2개임.\n",
    "이 리뷰를 lstm모델을 활용하여 긍정 혹은 부정으로 예측하는 모델을 구축할것임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 정제\n",
    "duplicates = IMDB[IMDB.duplicated()]  \n",
    "print(\"Duplicated Rows:\\n\", duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDB = IMDB.drop_duplicates(subset='review') # 중복되는 행 제거\n",
    "IMDB.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDB.loc[:, 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDB.loc[:, 'sentiment'] = IMDB['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "IMDB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: 텍스트 데이터 전처리\n",
    "# 모델의 input을 효율적으로하고 정확한 성능을 내기 위해서 텍스트데이터를 일관된 형식으로 변환하는 과정이 필요함.\n",
    "# 1. 텍스트 정규화 -> \"특수문자\", \"대/소문자\", \"두 칸 이상의 공백\" 제거 \n",
    "# 2. 자연어 처리에서 큰 의미를 가지지 않는다고 알려져 있는 불용어(stopword) 제거 \n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "import re\n",
    "def clean_text(text):\n",
    "    text = text.lower() # 소문자화\n",
    "    text = re.sub(r\"<.*?>\", \"\", text) # HTML태그 제거 \n",
    "    text = re.sub(r\"[^.a-z\\s!?']\", \"\", text) # 특수 문자 및 숫자 제거\n",
    "    text = \" \".join(word for word in text.split() if word not in stop_words) # 불용어 제거\n",
    "    text = re.sub(r'([!?\\'\"])\\1+', r'\\1', text) # !?'이 2개 이상이면 한 개로 만들어줌.\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # 불필요한 공백 제거\n",
    "    return text\n",
    "IMDB['review_cleaned'] = IMDB['review'].apply(clean_text)\n",
    "# pd.Series([len(review) for review in IMDB[\"review_cleaned\"].to_list()]).plot.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "IMDB.loc[:,'review_tokenized'] = IMDB.loc[:,'review_cleaned'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어집합 생성\n",
    "import numpy as np\n",
    "from nltk import FreqDist\n",
    "\n",
    "all_tokens = np.hstack(IMDB[\"review_tokenized\"])\n",
    "\n",
    "# 단어 집합 생성 및 빈도 계산\n",
    "vocab = FreqDist(all_tokens)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FREQ_THRESHOLD = 3\n",
    "vocab = {key: value for key, value in vocab.items() if value >= FREQ_THRESHOLD}\n",
    "\n",
    "# 단어와 인덱스 할당\n",
    "# 단어 인덱스를 2부터 시작하여 word2idx 생성 -> 레이블 데이터를 1과 0으로 설정 했기 때문\n",
    "word2idx = {word: idx + 2 for idx, (word, _) in enumerate(vocab.items())}\n",
    "word2idx[\"<pad>\"] = 0  # 패딩을 위한 인덱스 0 예약\n",
    "word2idx[\"<unk>\"] = 1  # 알 수 없는 단어를 위한 인덱스 1 예약\n",
    "VOCAB_SIZE = len(word2idx)\n",
    "\n",
    "VOCAB_SIZE, word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.util import save_pkl\n",
    "save_pkl(\"./word2index.pkl\",word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDB['review_tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 맵핑(단어 집합을 데이터에 적용)\n",
    "def word_to_num(word):\n",
    "    try:\n",
    "        return word2idx[word]  # 글자를 해당되는 정수로 변환\n",
    "    except KeyError:  # 단어 집합에 없는 단어일 경우 unk로 대체된다.\n",
    "        return word2idx[\"<unk>\"]  # unk의 인덱스로 변환\n",
    "\n",
    "\n",
    "IMDB[\"review_numbered\"] = IMDB[\"review_tokenized\"].apply(lambda _X: [word_to_num(word) for word in _X])\n",
    "IMDB[\"token_length\"] = IMDB[\"review_numbered\"].apply(lambda _X: len(_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDB[\"token_length\"].plot.hist(bins=30)\n",
    "max(IMDB[\"token_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKEN = 256\n",
    "IMDB_256 = IMDB[IMDB[\"token_length\"]<MAX_TOKEN]\n",
    "IMDB_256[\"token_length\"].plot.hist(bins=30)\n",
    "len(IMDB_256[\"token_length\"])/len(IMDB[\"token_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Valid Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_raw, y_raw = IMDB_256[\"review_numbered\"], IMDB_256[\"sentiment\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    x_raw, y_raw, test_size=0.8, random_state=0, stratify=y_raw\n",
    ")\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=0, stratify=y_train\n",
    ")\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 패딩 작업\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def seq_padding(sequence: pd.Series):\n",
    "    # 정수 인코딩된 시퀀스를 PyTorch 텐서로 변환\n",
    "    encoded_tensors = [torch.tensor(seq) for seq in sequence.to_list()]\n",
    "    # 패딩 적용 (최대 길이에 맞춰 0으로 패딩)\n",
    "    return pad_sequence(encoded_tensors, batch_first=True, padding_value=0, padding_side=\"left\")\n",
    "\n",
    "\n",
    "X_train, X_valid, X_test = seq_padding(X_train), seq_padding(X_valid), seq_padding(X_test)\n",
    "y_train, y_valid, y_test = y_train.apply(int), y_valid.apply(int), y_test.apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 패딩이 적용된 시퀀스와 레이블 합치기\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "def dataloader_gen(x, y, batch_size=BATCH_SIZE):\n",
    "    x = torch.tensor(x, dtype=torch.int32)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "    return DataLoader(TensorDataset(x, y), batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "dataloader = dataloader_gen(X_train, y_train.to_numpy())\n",
    "valid_dataloader = dataloader_gen(X_valid, y_valid.to_numpy())\n",
    "test_dataloader = dataloader_gen(X_test, y_test.to_numpy())\n",
    "\n",
    "\n",
    "# 배치를 확인하며 첫 번째 배치 출력\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(\"Padded Sequences:\\n\", inputs)\n",
    "    print(\"Labels:\\n\", targets)\n",
    "    break  # 첫 번째 배치만 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Param\n",
    "LEARNING_RATE = 0.00001\n",
    "N_EPOCHS = 100\n",
    "\n",
    "\n",
    "EMBED_SIZE = 256\n",
    "HIDDEN_SIZE = 1024\n",
    "OUTPUT_DIM = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_dim=VOCAB_SIZE,\n",
    "        embedding_dim=EMBED_SIZE,\n",
    "        hidden_dim=HIDDEN_SIZE,\n",
    "        output_dim=OUTPUT_DIM,\n",
    "        device=device,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_dim, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed = self.embed(x)\n",
    "        y_t_list, h_t_list = self.rnn(embed)\n",
    "        h_t = h_t_list.squeeze(0)\n",
    "\n",
    "        feature = self.fc1(h_t)\n",
    "        feature = F.relu(feature)\n",
    "        output = self.fc2(feature)\n",
    "        return self.softmax(output)\n",
    "\n",
    "\n",
    "# Training setup\n",
    "model = SentimentAnalysisRNN(vocab_dim=VOCAB_SIZE, device=device).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(logits, labels):\n",
    "    # _, predicted = torch.max(logits, 1)\n",
    "    predicted = torch.argmax(logits, dim=1)\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    total = labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "def evaluate(model, valid_dataloader, criterion, device):\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 데이터로더로부터 배치 크기만큼의 데이터를 연속으로 로드\n",
    "        for batch_X, batch_y in valid_dataloader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            # 모델의 예측값\n",
    "            logits = model(batch_X)\n",
    "\n",
    "            # 손실을 계산\n",
    "            loss = criterion(logits, batch_y)\n",
    "\n",
    "            # 정확도와 손실을 계산함\n",
    "            val_loss += loss.item()\n",
    "            val_correct += calculate_accuracy(logits, batch_y) * batch_y.size(0)\n",
    "            val_total += batch_y.size(0)\n",
    "\n",
    "    val_accuracy = val_correct / val_total\n",
    "    val_loss /= len(valid_dataloader)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainin_at = now()\n",
    "best_val_loss = float('inf')  # 검증 손실의 최저 값을 추적하기 위한 변수로, 초기값은 매우 큰 값으로 설정합니다.\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        x_batch, y_batch = samples\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        predicted = model(x_batch)\n",
    "        loss = loss_fn(predicted, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_correct += calculate_accuracy(predicted, y_batch) * y_batch.size(0)\n",
    "        train_total += y_batch.size(0)\n",
    "        \n",
    "    train_accuracy = train_correct / train_total\n",
    "    train_loss /= len(dataloader)\n",
    "    \n",
    "      # Validation\n",
    "    val_loss, val_accuracy = evaluate(model, valid_dataloader, loss_fn, device)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{N_EPOCHS}:')\n",
    "    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    # 검증 손실이 최소일 때 체크포인트 저장\n",
    "    if val_loss < best_val_loss:\n",
    "        print(f'Validation loss improved from {best_val_loss:.4f} to {val_loss:.4f}. 체크포인트를 저장합니다.')\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), f'best_model_checkpoint-{trainin_at}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_json(\n",
    "    f\"./training_param_{trainin_at}.json\",\n",
    "    {\n",
    "        \"voca_frequency_thresold\": FREQ_THRESHOLD,\n",
    "        \"token_truncation\": MAX_TOKEN,\n",
    "        \"lr\": LEARNING_RATE,\n",
    "        \"voca_size\":VOCAB_SIZE,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"embed_dim\": EMBED_SIZE,\n",
    "        \"rnn_hidden_dim\": HIDDEN_SIZE,\n",
    "        \"output_dim\": OUTPUT_DIM,\n",
    "        \"loss\":best_val_loss\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model,test_dataloader,loss_fn,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentLSTM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
